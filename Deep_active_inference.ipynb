{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vegsa/DAI_test/blob/main/Deep_active_inference.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "VuA5YBXu3n3Q"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import math\n",
        "import gym\n",
        "from gym import spaces\n",
        "from gym.utils import seeding\n",
        "import copy\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Environment \n",
        "class GridEnv(gym.Env):\n",
        "    \n",
        "    metadata = {'render.modes': ['humans']}\n",
        "    \n",
        "    def __init__(self, custom_map):\n",
        "        super(GridEnv, self).__init__()\n",
        "\n",
        "        # 0: 'W', 1: 'N' 2: 'E', 3: 'S', 4: 'SW', 5: 'NW', 6: 'NE', 7: 'SE', 8: Stay\n",
        "        self.actions = [0, 1, 2, 3, 4, 5, 6, 7, 8]\n",
        "        \n",
        "        #Action space\n",
        "        self.action_space = spaces.Discrete(len(self.actions))\n",
        "        \n",
        "        self.actions_pos_dict = {0: [0, -1], 1: [-1, 0], 2: [0, 1], 3: [1, 0], 4: [1, -1], 5: [-1, -1], 6: [-1, 1], 7: [1, 1], 8: [0, 0]}\n",
        "        \n",
        "        # Observation space\n",
        "        #self.observation_space = spaces.Box(low=0, high=1, shape=(100,100), dtype=np.float32)\n",
        "        \n",
        "        self.n_width = len(custom_map[0])\n",
        "        self.n_height = len(custom_map)\n",
        "        \n",
        "        self.observation_space = spaces.Discrete(self.n_width*self.n_height)\n",
        "        \n",
        "        self.start_state = (0,0) #(0,0)\n",
        "        \n",
        "        #self.goal_state = (17,13)  #(5,7) #(26,24) #(self.n_width-1,self.n_height-1)\n",
        "        \n",
        "        self.agent_state = copy.deepcopy(self.start_state)\n",
        "                \n",
        "        #States that cannot be hit\n",
        "        self.obstacles, self.goal_state = self.get_obstacles_and_goal(custom_map)\n",
        "\n",
        "        # Rewards: Goal, Fail, Energy, Closer to goal both direction, Closer to goal one direction, Stay\n",
        "        self.rewards = [1.15, -2, -0.5, 0.7, 0.25, -0.5] #[1, -1, -0.1, 0.1, 0.05, -0.1] #[75, -75, -15, 10, 3, -15] #[5, -1, -0.1, 0.2, 0.1, -0.1] \n",
        "        # [5,-3,-0.1, 0.1, 0.05, -0.5]\n",
        "        self.reset()\n",
        "        self.seed()\n",
        "        \n",
        "        # Seeding\n",
        "    def seed(self, seed=None):\n",
        "        self.np_random, seed = seeding.np_random(seed)\n",
        "        return [seed]\n",
        "        \n",
        "    def step(self, action):\n",
        "        next_x, next_y = (self.agent_state[0] + self.actions_pos_dict[action][0],\n",
        "                          self.agent_state[1] + self.actions_pos_dict[action][1])\n",
        "        \n",
        "        if next_x < 0:\n",
        "            next_x = 0\n",
        "        \n",
        "        if next_x >= self.n_width:\n",
        "            next_x = self.n_width-1\n",
        "            \n",
        "        if next_y < 0:\n",
        "            next_y = 0\n",
        "            \n",
        "        if next_y >= self.n_height:\n",
        "            next_y = self.n_height-1\n",
        "            \n",
        "        reward, done = self.get_reward(next_x, next_y)\n",
        "        self.agent_state = next_x, next_y\n",
        "\n",
        "        info ={'X: ': next_x, 'Y: ': next_y}\n",
        "        \n",
        "        return self.agent_state, reward, done, info\n",
        "            \n",
        "        \n",
        "    def reset(self, start_state=(0,0)):\n",
        "        self.agent_state = start_state\n",
        "        return self.agent_state\n",
        "        \n",
        "    def render(self, mode='human'):\n",
        "        pass\n",
        "    \n",
        "    def close(self):\n",
        "        pass\n",
        "\n",
        "    def xy_to_state(self, x, y):\n",
        "        return y + self.n_width*x\n",
        "    \n",
        "    def state_to_xy(self, state):\n",
        "        x = state//self.n_width\n",
        "        y = state % self.n_width\n",
        "        return x, y\n",
        "    \n",
        "    def get_reward(self, x, y):\n",
        "        i, done = self.get_state_status(x,y)\n",
        "        return self.rewards[i], done\n",
        "    \n",
        "    def get_state_status(self, x, y):\n",
        "        \n",
        "        if (x,y) in self.obstacles:\n",
        "            return 1, True\n",
        "        elif (x,y) == self.goal_state:\n",
        "            return 0, True\n",
        "        elif self.closer_to_goal(x,y):\n",
        "            return 3, False\n",
        "        elif self.closer_to_goal2(x,y):\n",
        "            return 4, False\n",
        "        elif (x,y) == self.agent_state:\n",
        "            return 5, False\n",
        "        else:\n",
        "            return 2, False\n",
        "        \n",
        "    def get_obstacles_and_goal(self, custom_map):\n",
        "        arr = []\n",
        "        goal = (0,0)\n",
        "        for i in range(self.n_height):\n",
        "            for j in range(self.n_width):\n",
        "                if custom_map[i,j] == 1:\n",
        "                    arr.append((i,j))\n",
        "\n",
        "                if custom_map[i,j] == 2:\n",
        "                    goal = (i,j)\n",
        "\n",
        "        print(\"Obstacles: \", arr)\n",
        "        return arr, goal\n",
        "\n",
        "    # Fix these two ways of assigning rewards when moving towards the goal.\n",
        "    def closer_to_goal(self, new_x, new_y):\n",
        "        x_old = abs(self.goal_state[0] - self.agent_state[0])\n",
        "        y_old = abs(self.goal_state[1] - self.agent_state[1])\n",
        "\n",
        "        x_new = abs(self.goal_state[0] - new_x)\n",
        "        y_new = abs(self.goal_state[1] - new_y)\n",
        "        \n",
        "        if x_new < x_old and y_new < y_old:\n",
        "            return True\n",
        "        else:\n",
        "            return False\n",
        "    \n",
        "    def closer_to_goal2(self, new_x, new_y):\n",
        "        \n",
        "        old = abs(self.goal_state[0] - self.agent_state[0]) + abs(self.goal_state[1] - self.agent_state[1])\n",
        "        new = abs(self.goal_state[0] - new_x) + abs(self.goal_state[1] - new_y)\n",
        "        \n",
        "        if new < old:\n",
        "            return True\n",
        "        else:\n",
        "            return False\n",
        "            \n",
        "    def return_goal_pos(self):\n",
        "        #return self.xy_to_state(self.goal_state[0], self.goal_state[1])\n",
        "        return self.goal_state\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "bi36RoORTQCt"
      },
      "outputs": [],
      "source": [
        "# Goal: (26, 24) / (6,24)\n",
        "\"\"\"\n",
        "custom_map = np.array([\n",
        "        [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\n",
        "        [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\n",
        "        [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\n",
        "        [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\n",
        "        [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\n",
        "        [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\n",
        "        [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\n",
        "        [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\n",
        "        [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\n",
        "        [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1],\n",
        "        [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1],\n",
        "        [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1],\n",
        "        [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1],\n",
        "        [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1],\n",
        "        [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1],\n",
        "        [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,1,1,1,1,1,1],\n",
        "        [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,1,1,1,1,1,1,1],\n",
        "        [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,1,1,1,1,1,1,1],\n",
        "        [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,1,1,1,1,1,1],\n",
        "        [0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,0,0,1,0,0,0,1,1,1,1,1,1],\n",
        "        [0,0,0,0,0,0,1,1,1,0,0,0,0,1,1,1,1,0,0,0,1,1,0,0,0,1,1,1,1,1],\n",
        "        [0,0,0,0,0,0,1,1,1,1,0,0,1,1,1,1,1,0,0,1,1,0,0,0,0,0,1,1,1,1],\n",
        "        [0,0,0,0,0,1,1,1,1,0,0,0,1,1,1,1,0,0,0,1,0,0,0,1,0,0,0,1,1,1],\n",
        "        [0,0,0,0,0,1,1,1,1,0,0,1,1,1,1,1,0,0,1,1,0,0,1,1,0,0,0,1,1,1],\n",
        "        [0,0,0,0,0,1,1,1,1,0,0,1,1,1,1,0,0,0,1,0,0,0,1,0,0,0,1,1,1,1],\n",
        "        [0,0,0,0,1,1,1,1,0,0,0,1,1,1,1,0,0,1,1,0,0,1,1,0,0,1,1,1,1,1],\n",
        "        [0,0,0,0,1,1,1,1,1,1,1,1,1,1,1,0,0,1,1,0,0,1,0,0,2,1,1,1,1,1],\n",
        "        [0,0,1,1,1,1,1,1,1,1,1,1,1,1,0,0,0,1,1,1,1,1,0,0,0,1,1,1,1,1],\n",
        "        [0,0,1,1,1,1,1,1,1,1,1,1,1,1,0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1],\n",
        "        [0,1,1,1,1,1,1,1,1,1,1,1,1,0,0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1]\n",
        "    ])\n",
        "\"\"\"\n",
        "custom_map = np.array([\n",
        "        [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\n",
        "        [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\n",
        "        [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\n",
        "        [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\n",
        "        [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\n",
        "        [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\n",
        "        [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\n",
        "        [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\n",
        "        [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\n",
        "        [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1],\n",
        "        [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1],\n",
        "        [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1],\n",
        "        [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1],\n",
        "        [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1],\n",
        "        [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1],\n",
        "        [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1],\n",
        "        [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1],\n",
        "        [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1],\n",
        "        [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1],\n",
        "        [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1],\n",
        "        [0,0,0,0,0,0,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1],\n",
        "        [0,0,0,0,0,0,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1],\n",
        "        [0,0,0,0,0,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1],\n",
        "        [0,0,0,0,0,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1],\n",
        "        [0,0,0,0,0,1,1,1,1,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1],\n",
        "        [0,0,0,0,1,1,1,1,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1],\n",
        "        [0,0,0,0,1,1,1,1,1,1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,2,1,1,1,1,1],\n",
        "        [0,0,1,1,1,1,1,1,1,1,1,1,1,1,0,0,0,1,1,1,1,1,0,0,0,1,1,1,1,1],\n",
        "        [0,0,1,1,1,1,1,1,1,1,1,1,1,1,0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1],\n",
        "        [0,1,1,1,1,1,1,1,1,1,1,1,1,0,0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1]\n",
        "    ])\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "#(17,13)\n",
        "custom_map = np.array([\n",
        "        [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\n",
        "        [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\n",
        "        [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\n",
        "        [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\n",
        "        [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\n",
        "        [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\n",
        "        [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\n",
        "        [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\n",
        "        [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\n",
        "        [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1],\n",
        "        [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1],\n",
        "        [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1],\n",
        "        [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1],\n",
        "        [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1],\n",
        "        [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1],\n",
        "        [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1],\n",
        "        [0,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1],\n",
        "        [0,1,1,1,1,0,0,0,0,0,0,0,0,2,0,1,1,1,1,1],\n",
        "        [1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1],\n",
        "        [1,1,1,0,0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1]\n",
        "    ])\n",
        "\n",
        "custom_map = np.array([\n",
        "        [0,0,0,0,0,0,0,0,0,0],\n",
        "        [0,0,0,0,0,1,0,0,0,0],\n",
        "        [0,0,0,0,0,0,0,0,1,0],\n",
        "        [0,0,0,0,0,0,0,0,0,0],\n",
        "        [0,0,0,0,0,0,0,0,0,0],\n",
        "        [1,0,0,0,0,0,0,2,0,0],\n",
        "        [0,0,0,0,0,0,0,0,0,1],\n",
        "        [0,0,0,1,0,0,0,0,0,0],\n",
        "        [0,0,0,0,0,0,0,0,0,0],\n",
        "        [1,0,0,0,0,1,0,0,0,1],\n",
        "    ])\n",
        "\"\"\"\n",
        "N_WIDTH = 30\n",
        "N_HEIGHT = 30"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "SgIRNu7ni0U4"
      },
      "outputs": [],
      "source": [
        "EPS_VAL = 1e-16\n",
        "def log_stable2(arr):\n",
        "    return np.log(arr + EPS_VAL)\n",
        "\n",
        "def KL_divergence(q, p):\n",
        "    KL = []\n",
        "    for i in range(len(q)):\n",
        "        KL_i =  np.sum(q[i]*(log_stable2(q[i]) - log_stable2(p[i])))\n",
        "        KL.append([KL_i])\n",
        "        \n",
        "        #print(\"q: \", q[i], \"p \", p[i], \"KL: \", KL_i)\n",
        "    return KL\n",
        "\n",
        "def softmax(x):\n",
        "    e_x = np.exp(x - np.max(x))\n",
        "    return e_x / e_x.sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "s89yalW-Un7f"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from collections import deque\n",
        "\n",
        "\n",
        "class Model(nn.Module):\n",
        "\n",
        "      def __init__(self, num_inputs, num_outputs, num_hidden=200, learning_rate=1e-3, softmax=False, relu_output=False, device='cpu'):  # device='cpu'\n",
        "          super(Model, self).__init__()\n",
        "\n",
        "          self.num_inputs = num_inputs\n",
        "          self.num_hidden = num_hidden\n",
        "          self.num_outputs = num_outputs\n",
        "          self.softmax = softmax\n",
        "          self.relu_output = relu_output\n",
        "\n",
        "          self.fc1 = nn.Linear(self.num_inputs, self.num_hidden)  #Hidden layer\n",
        "          self.fc12 = nn.Linear(self.num_hidden, self.num_hidden)\n",
        "          #self.fc23 = nn.Linear(self.num_hidden, self.num_hidden)\n",
        "          self.fc2 = nn.Linear(self.num_hidden, self.num_outputs) #Output layer\n",
        "\n",
        "          self.optimizer = optim.Adam(self.parameters(), lr=learning_rate)\n",
        "          self.device = device\n",
        "          self.to(self.device)\n",
        "\n",
        "      def forward(self, x):\n",
        "\n",
        "          layer1 = F.relu(self.fc1(x))\n",
        "          layer2 = F.relu(self.fc12(layer1))\n",
        "          #layer3 = F.relu(self.fc23(layer2))\n",
        "          #output = self.fc2(layer2)\n",
        "          if self.softmax:\n",
        "              output = F.softmax(self.fc2(layer2), dim=-1).clamp(min=1e-9, max=1-1e-9)\n",
        "          elif self.relu_output:\n",
        "              output = F.relu(self.fc2(layer2))\n",
        "          else:\n",
        "              output = self.fc2(layer2)\n",
        "          return output\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "id": "f8ieG1CBEDno"
      },
      "outputs": [],
      "source": [
        "class Agent():\n",
        "\n",
        "    def __init__(self, env):\n",
        "\n",
        "        # Parameters of the environment\n",
        "        self.obs_size = 2\n",
        "        self.num_actions = env.action_space.n\n",
        "\n",
        "        # Networks\n",
        "        \n",
        "        self.transition_net = Model(self.obs_size+1, self.obs_size, num_hidden=64, learning_rate=1e-3, softmax=False, relu_output=True)\n",
        "        self.policy_net = Model(self.obs_size,self.num_actions, num_hidden=64, learning_rate=1e-3, softmax=True)    \n",
        "        self.EFE_net = Model(self.obs_size,self.num_actions, num_hidden=64, learning_rate=1e-4, softmax=False) \n",
        "        self.EFE_target_net = copy.deepcopy(self.EFE_net)\n",
        "        \"\"\"\n",
        "        self.transition_net, self.transnet_optimizer = self.network_init(self.obs_size+1,self.obs_size, num_hidden=200, softmax=False)\n",
        "        self.policy_net, self.polnet_optimizer = self.network_init(self.obs_size,self.num_actions, num_hidden=200, softmax=True)    \n",
        "        self.EFE_net, self.EFEnet_optimizer = self.network_init(self.obs_size,self.num_actions, num_hidden=200, softmax=False) \n",
        "        self.EFE_target_net = copy.deepcopy(self.EFE_net)\n",
        "        \"\"\"\n",
        "\n",
        "        # Parameters for training the networks\n",
        "        self.gamma = 0.9\n",
        "        self.Beta = 0.99\n",
        "        self.epsilon = 1.0\n",
        "        self.min_explore = 0.001\n",
        "\n",
        "        self.mem_size = 65536\n",
        "        self.batch_size = 64\n",
        "        self.max_moves = 1000\n",
        "        self.sync_freq = 25\n",
        "\n",
        "        self.step_count = 0\n",
        "        self.steps = 0\n",
        "\n",
        "        # Replay memory\n",
        "        self.replay_mem = deque(maxlen=self.mem_size)\n",
        "\n",
        "        self.EPS_value = 1e-16\n",
        "\n",
        "\n",
        "    # Sample action from policy net and take it\n",
        "    def sample_action(self, obs):\n",
        "        \n",
        "        if np.random.uniform(0,1) < self.epsilon:\n",
        "            action = np.random.randint(0,9)\n",
        "            #action = np.random.randint(0,16)\n",
        "            #return torch.tensor([[action]])\n",
        "        else:\n",
        "            with torch.no_grad():\n",
        "                policy = self.policy_net(obs)\n",
        "                #efe = self.EFE_net(obs)\n",
        "                #soft_efe = torch.softmax(-self.gamma*efe, dim=1).clamp(min=1e-9, max=1-1e-9)\n",
        "                #return torch.multinomial(policy, 1)\n",
        "                #print(\"policy in \",obs ,\": \",policy)\n",
        "                #print(\"p(a|s) in \", obs, \": \", soft_efe)\n",
        "            action = np.argmax(policy.data.numpy())\n",
        "        \n",
        "        \"\"\"\n",
        "        with torch.no_grad():\n",
        "            # Determine the action distribution given the current observation:\n",
        "            policy = self.policy_net(obs)\n",
        "            #print(\"Observation: \", obs)\n",
        "            #print(\"Policy: \", policy)\n",
        "            return torch.multinomial(policy, 1)\n",
        "        \"\"\"\n",
        "        return torch.tensor([[action]])\n",
        "    \n",
        "    def network_init(self, num_input, num_output, num_hidden=64, softmax=False):\n",
        "      \n",
        "        layer1 = num_input\n",
        "        layer2 = num_hidden\n",
        "        layer3 = num_hidden\n",
        "        layer4 = num_output\n",
        "\n",
        "        if softmax:\n",
        "            model = torch.nn.Sequential(\n",
        "                torch.nn.Linear(layer1, layer2),\n",
        "                torch.nn.Tanh(),\n",
        "                torch.nn.Linear(layer2, layer3),\n",
        "                torch.nn.Tanh(),\n",
        "                torch.nn.Linear(layer3, layer4),\n",
        "                torch.nn.Softmax(dim=1)\n",
        "                )\n",
        "        else:\n",
        "            model = torch.nn.Sequential(\n",
        "                torch.nn.Linear(layer1, layer2),\n",
        "                torch.nn.Tanh(),\n",
        "                torch.nn.Linear(layer2, layer3),\n",
        "                torch.nn.Tanh(),\n",
        "                torch.nn.Linear(layer3, layer4)\n",
        "                )\n",
        "\n",
        "        learning_rate = 0.001\n",
        "        optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)    \n",
        "        \n",
        "        return model, optimizer\n",
        "\n",
        "    def get_batches(self):\n",
        "        minibatch = random.sample(self.replay_mem, self.batch_size)\n",
        "        \n",
        "        obs_t0_batch = torch.cat([s0 for (s0,a0,s1,r1,a1,s2,d) in minibatch])\n",
        "        #print(\"obs_t0: \", obs_t0_batch)\n",
        "        action_t0_batch = torch.cat([a0 for (s0,a0,s1,r1,a1,s2,d) in minibatch])\n",
        "        #print(\"action_t0: \", action_t0_batch)\n",
        "        obs_t1_batch = torch.cat([s1 for (s0,a0,s1,r1,a1,s2,d) in minibatch])\n",
        "        #print(\"obs_t1: \", obs_t1_batch)\n",
        "        reward_t1_batch = torch.Tensor([r1 for (s0,a0,s1,r1,a1,s2,d) in minibatch])\n",
        "        reward_t1_batch = reward_t1_batch.expand(1,self.batch_size)\n",
        "        reward_t1_batch = torch.transpose(reward_t1_batch,0,1)\n",
        "        #print(\"reward_t1: \", reward_t1_batch)\n",
        "\n",
        "        action_t1_batch = torch.cat([a1 for (s0,a0,s1,r1,a1,s2,d) in minibatch])\n",
        "        #print(\"action_t1: \", action_t1_batch)\n",
        "        obs_t2_batch = torch.cat([s2 for (s0,a0,s1,r1,a1,s2,d) in minibatch])\n",
        "        #print(\"obs_t2: \", obs_t2_batch)\n",
        "        done_batch = torch.Tensor([d for (s0,a0,s1,r1,a1,s2,d) in minibatch])\n",
        "        done_batch = done_batch.expand(1,self.batch_size)\n",
        "        done_batch = torch.transpose(done_batch,0,1)\n",
        "        #print(\"done_batch: \", done_batch)\n",
        "        \n",
        "        state_and_action = torch.cat((obs_t0_batch, action_t0_batch.float()),dim=1) #\n",
        "        pred_state_t1_batch = self.transition_net(state_and_action) # Predict the next state with the transition network\n",
        "        pred_state_error_t1 = torch.mean(F.mse_loss(pred_state_t1_batch, obs_t1_batch, reduction='none'),dim=1).unsqueeze(1)\n",
        "        #print(\"---\")\n",
        "        #print(\"pred_error: \", pred_state_t1_batch)\n",
        "        #print(\"obs_t1: \", obs_t1_batch)\n",
        "\n",
        "\n",
        "        pred_state_nump = pred_state_t1_batch.detach().numpy()\n",
        "        obs_t1_nump = obs_t1_batch.detach().numpy()\n",
        "\n",
        "        #KL = KL_divergence(pred_state_nump, obs_t1_nump)\n",
        "        #KL = torch.tensor(KL)\n",
        "        #print(\"D_KL\", KL)\n",
        "\n",
        "        return obs_t0_batch, action_t0_batch, obs_t1_batch, reward_t1_batch, action_t1_batch, obs_t2_batch, done_batch, pred_state_error_t1    #, KL\n",
        "\n",
        "    def compute_EFE_net_loss(self,obs_t1_batch, reward_t1_batch, action_t0_batch, action_t1_batch, obs_t2_batch, done_batch, pred_state_error_t1):\n",
        "\n",
        "        with torch.no_grad():\n",
        "            policy_t2_batch = self.policy_net(obs_t2_batch)\n",
        "            target_EFE_t2_batch = self.EFE_target_net(obs_t2_batch)\n",
        "            weighted_targets = ((1-done_batch)*policy_t2_batch*target_EFE_t2_batch).sum(-1).unsqueeze(1)\n",
        "            EFE_estimate_batch = -reward_t1_batch + pred_state_error_t1 + self.Beta*weighted_targets #KL\n",
        "        #print(\"EFE_est: \", EFE_estimate_batch)\n",
        "        #print(\"reward_t1: \", reward_t1_batch)\n",
        "        #print(\"pred_state_error: \", pred_state_error_t1)\n",
        "        #print(\"weights: \", weighted_targets)\n",
        "        #print(\"obs_t1: \", obs_t1_batch)\n",
        "        EFE_t1_batch = self.EFE_net(obs_t1_batch).gather(1,action_t1_batch) # Check this out\n",
        "        \n",
        "        #print(\"obs_t1: \", obs_t1_batch)\n",
        "        #print(\"EFE_t1_batch: \", EFE_t1_batch)\n",
        "        \n",
        "        L = F.mse_loss(EFE_estimate_batch, EFE_t1_batch)\n",
        "        return L\n",
        "\n",
        "    def compute_VFE(self, obs_t1_batch, pred_error_batch):\n",
        "\n",
        "        policy_t1_batch = self.policy_net(obs_t1_batch)\n",
        "\n",
        "        EFE_t1_batch = self.EFE_net(obs_t1_batch).detach()\n",
        "\n",
        "        softmax_EFE_t1_batch = torch.softmax(-self.gamma*EFE_t1_batch, dim=1).clamp(min=1e-9, max=1-1e-9) #clamp\n",
        "\n",
        "        energy_batch = -(policy_t1_batch*torch.log(softmax_EFE_t1_batch)).sum(-1).view(self.batch_size,1)\n",
        "\n",
        "        entropy_batch = -(policy_t1_batch*torch.log(policy_t1_batch)).sum(-1).view(self.batch_size,1)\n",
        "\n",
        "        VFE_batch = pred_error_batch + (energy_batch - entropy_batch)\n",
        "        #VFE_batch = KL + (energy_batch - entropy_batch)\n",
        "\n",
        "        VFE = torch.mean(VFE_batch)\n",
        "        return VFE\n",
        "    \n",
        "    def learn(self):\n",
        "\n",
        "        if self.step_count % self.sync_freq == 0:\n",
        "            self.EFE_target_net.load_state_dict(self.EFE_net.state_dict())\n",
        "\n",
        "        obs_t0_batch, action_t0_batch, obs_t1_batch, reward_t1_batch, action_t1_batch, obs_t2_batch, done_batch, pred_state_error_t1 = self.get_batches()\n",
        "\n",
        "        EFE_loss = self.compute_EFE_net_loss(obs_t1_batch, reward_t1_batch, action_t0_batch, action_t1_batch, obs_t2_batch, done_batch, pred_state_error_t1)\n",
        "        #print(\"EFE_loss: \", EFE_loss)\n",
        "\n",
        "        VFE = self.compute_VFE(obs_t1_batch, pred_state_error_t1)\n",
        "        \n",
        "        self.transition_net.optimizer.zero_grad()\n",
        "        self.policy_net.optimizer.zero_grad()\n",
        "        self.EFE_net.optimizer.zero_grad()\n",
        "        \"\"\"\n",
        "        self.transnet_optimizer.zero_grad()\n",
        "        self.polnet_optimizer.zero_grad()\n",
        "        self.EFEnet_optimizer.zero_grad()\n",
        "\n",
        "        VFE.backward()    \n",
        "        EFE_loss.backward()  \n",
        "\n",
        "        self.transnet_optimizer.step()\n",
        "        self.polnet_optimizer.step()\n",
        "        self.EFEnet_optimizer.step()\n",
        "        \"\"\"\n",
        "        VFE.backward()    \n",
        "        EFE_loss.backward()  \n",
        "\n",
        "        self.transition_net.optimizer.step()\n",
        "        self.policy_net.optimizer.step()\n",
        "        self.EFE_net.optimizer.step()\n",
        "        \n",
        "    def train_networks(self, episodes):\n",
        "        rewards = []\n",
        "        goal_count = 0\n",
        "        counter = 0\n",
        "        epsilon_adjust = 1\n",
        "        epsilon_reset = False\n",
        "\n",
        "        for i in range(episodes):\n",
        "            print(\"---------------Episode: \", i, \"---------------\")\n",
        "            \"\"\"\n",
        "            if goal_count < 100:\n",
        "                obs_t0 = env.reset((24,24))\n",
        "            #elif goal_count < 100:\n",
        "            #    obs_t0 = env.reset((21,24))\n",
        "            elif goal_count < 200:\n",
        "                obs_t0 = env.reset((19,22))\n",
        "            #elif goal_count < 200:\n",
        "            #    obs_t0 = env.reset((17,21))\n",
        "            elif goal_count < 300:\n",
        "                obs_t0 = env.reset((14,20))\n",
        "            #elif goal_count < 300:\n",
        "            #    obs_t0 = env.reset((10,15))\n",
        "            else:\n",
        "                obs_t0 = env.reset()\n",
        "            \"\"\"\n",
        "            obs_t0 = env.reset()\n",
        "            obs_t0 = torch.from_numpy(np.array([obs_t0[0], obs_t0[1]]).reshape(1,self.obs_size)).float()\n",
        "            #obs_t0 = torch.tensor(obs_t0, dtype=torch.float32, device='cpu')\n",
        "            \n",
        "\n",
        "            action_t0 = self.sample_action(obs_t0)\n",
        "\n",
        "            obs_t1, reward_t1, done1, _ = env.step(action_t0[0].item())  #done1\n",
        "            obs_t1 = torch.from_numpy(np.array([obs_t1[0], obs_t1[1]]).reshape(1,self.obs_size)).float()\n",
        "            #done = False\n",
        "            obs_t2 = obs_t1\n",
        "            self.steps = 0\n",
        "            while (not done1) and (self.steps < self.max_moves): #done1\n",
        "                #print(\"Step \", steps)\n",
        "                self.steps += 1\n",
        "                \"\"\"\n",
        "                action_t0 = self.sample_action(obs_t0)\n",
        "\n",
        "                obs_t1, reward_t1, done_t1, _ = env.step(actiont_0[0].item)\n",
        "                obs_t1 = torch.from_numpy(np.array([obs_t1[0], obs_t1[1]]).reshape(1,layer1)).float()\n",
        "                \"\"\"\n",
        "\n",
        "                action_t1 = self.sample_action(obs_t1)\n",
        "                \n",
        "                #print(\"actiont1: \", action_t1[0].item())\n",
        "                obs_t2, reward_t2, done2, _ = env.step(action_t1[0].item()) # done2\n",
        "                #print(\"Obs_t2: \", obs_t2)\n",
        "                obs_t2 = torch.from_numpy(np.array([obs_t2[0], obs_t2[1]]).reshape(1,self.obs_size)).float()\n",
        "\n",
        "                obs_check = obs_t2.numpy()[0]\n",
        "                obs_check_nump = (int(obs_check[0]), int(obs_check[1]))\n",
        "                #print(\"obs_check: \", obs_check_nump)\n",
        "\n",
        "                exp = (obs_t0, action_t0, obs_t1, reward_t1, action_t1, obs_t2, done1) #done1\n",
        "                self.replay_mem.append(exp)\n",
        "\n",
        "                self.step_count += 1\n",
        "\n",
        "                if len(self.replay_mem) > self.batch_size:\n",
        "                    self.learn()\n",
        "\n",
        "                obs_t0 = obs_t1\n",
        "                obs_t1 = obs_t2\n",
        "\n",
        "                reward_t1 = reward_t2\n",
        "                action_t0 = action_t1\n",
        "                done1 = done2\n",
        "                if done1: #done1\n",
        "                    #exp = (obs_t0, action_t0, obs_t1, reward_t1, torch.tensor([[8]]), obs_t1, done1) #done1\n",
        "                    exp = (obs_t0, action_t0, obs_t1, reward_t1, action_t0, obs_t1, done1)\n",
        "                    self.replay_mem.append(exp)\n",
        "\n",
        "            observ = obs_t2.numpy()[0]\n",
        "            obs_to_nump = (int(observ[0]), int(observ[1]))\n",
        "            if obs_to_nump == env.return_goal_pos():\n",
        "                goal_count += 1\n",
        "            print(\"End state: \", obs_to_nump)\n",
        "            print(\"Steps: \", self.steps)\n",
        "            print(\"Goal count: \", goal_count)\n",
        "            \n",
        "            self.epsilon = max(self.min_explore, np.exp(-0.002*counter))\n",
        "            print(\"Epsilon: \", self.epsilon)\n",
        "            counter += 1\n",
        "            \"\"\"\n",
        "            #if counter % 1000 == 0:\n",
        "            if (goal_count % 100 == 0) and (goal_count <= 300) and epsilon_reset:\n",
        "                counter = 0\n",
        "                epsilon_reset=False\n",
        "            \n",
        "            if (goal_count % 100 == 1):\n",
        "                epsilon_reset = True\n",
        "            \"\"\"\n",
        "    \n",
        "        return self.policy_net\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XQBJA0K-unn7",
        "outputId": "16708e97-4715-42ed-aa2a-fdd524563efb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Obstacles:  [(9, 29), (10, 28), (10, 29), (11, 26), (11, 27), (11, 28), (11, 29), (12, 25), (12, 26), (12, 27), (12, 28), (12, 29), (13, 25), (13, 26), (13, 27), (13, 28), (13, 29), (14, 24), (14, 25), (14, 26), (14, 27), (14, 28), (14, 29), (15, 23), (15, 24), (15, 25), (15, 26), (15, 27), (15, 28), (15, 29), (16, 23), (16, 24), (16, 25), (16, 26), (16, 27), (16, 28), (16, 29), (17, 23), (17, 24), (17, 25), (17, 26), (17, 27), (17, 28), (17, 29), (18, 24), (18, 25), (18, 26), (18, 27), (18, 28), (18, 29), (19, 24), (19, 25), (19, 26), (19, 27), (19, 28), (19, 29), (20, 6), (20, 7), (20, 8), (20, 25), (20, 26), (20, 27), (20, 28), (20, 29), (21, 6), (21, 7), (21, 8), (21, 9), (21, 26), (21, 27), (21, 28), (21, 29), (22, 5), (22, 6), (22, 7), (22, 8), (22, 27), (22, 28), (22, 29), (23, 5), (23, 6), (23, 7), (23, 8), (23, 27), (23, 28), (23, 29), (24, 5), (24, 6), (24, 7), (24, 8), (24, 11), (24, 12), (24, 26), (24, 27), (24, 28), (24, 29), (25, 4), (25, 5), (25, 6), (25, 7), (25, 11), (25, 12), (25, 25), (25, 26), (25, 27), (25, 28), (25, 29), (26, 4), (26, 5), (26, 6), (26, 7), (26, 8), (26, 9), (26, 10), (26, 11), (26, 12), (26, 13), (26, 25), (26, 26), (26, 27), (26, 28), (26, 29), (27, 2), (27, 3), (27, 4), (27, 5), (27, 6), (27, 7), (27, 8), (27, 9), (27, 10), (27, 11), (27, 12), (27, 13), (27, 17), (27, 18), (27, 19), (27, 20), (27, 21), (27, 25), (27, 26), (27, 27), (27, 28), (27, 29), (28, 2), (28, 3), (28, 4), (28, 5), (28, 6), (28, 7), (28, 8), (28, 9), (28, 10), (28, 11), (28, 12), (28, 13), (28, 16), (28, 17), (28, 18), (28, 19), (28, 20), (28, 21), (28, 22), (28, 23), (28, 24), (28, 25), (28, 26), (28, 27), (28, 28), (28, 29), (29, 1), (29, 2), (29, 3), (29, 4), (29, 5), (29, 6), (29, 7), (29, 8), (29, 9), (29, 10), (29, 11), (29, 12), (29, 16), (29, 17), (29, 18), (29, 19), (29, 20), (29, 21), (29, 22), (29, 23), (29, 24), (29, 25), (29, 26), (29, 27), (29, 28), (29, 29)]\n",
            "---------------Episode:  0 ---------------\n",
            "End state:  (24, 11)\n",
            "Steps:  293\n",
            "Goal count:  0\n",
            "Epsilon:  1.0\n",
            "---------------Episode:  1 ---------------\n",
            "End state:  (22, 5)\n",
            "Steps:  774\n",
            "Goal count:  0\n",
            "Epsilon:  0.9980019986673331\n",
            "---------------Episode:  2 ---------------\n",
            "End state:  (17, 23)\n",
            "Steps:  401\n",
            "Goal count:  0\n",
            "Epsilon:  0.9960079893439915\n",
            "---------------Episode:  3 ---------------\n",
            "End state:  (16, 23)\n",
            "Steps:  558\n",
            "Goal count:  0\n",
            "Epsilon:  0.9940179640539353\n",
            "---------------Episode:  4 ---------------\n",
            "End state:  (24, 8)\n",
            "Steps:  330\n",
            "Goal count:  0\n",
            "Epsilon:  0.9920319148370607\n",
            "---------------Episode:  5 ---------------\n",
            "End state:  (21, 9)\n",
            "Steps:  949\n",
            "Goal count:  0\n",
            "Epsilon:  0.9900498337491681\n",
            "---------------Episode:  6 ---------------\n",
            "End state:  (24, 5)\n",
            "Steps:  210\n",
            "Goal count:  0\n",
            "Epsilon:  0.9880717128619305\n",
            "---------------Episode:  7 ---------------\n",
            "End state:  (20, 8)\n",
            "Steps:  198\n",
            "Goal count:  0\n",
            "Epsilon:  0.9860975442628619\n",
            "---------------Episode:  8 ---------------\n",
            "End state:  (2, 12)\n",
            "Steps:  1000\n",
            "Goal count:  0\n",
            "Epsilon:  0.9841273200552851\n",
            "---------------Episode:  9 ---------------\n",
            "End state:  (0, 0)\n",
            "Steps:  1000\n",
            "Goal count:  0\n",
            "Epsilon:  0.9821610323583008\n",
            "---------------Episode:  10 ---------------\n",
            "End state:  (2, 8)\n",
            "Steps:  1000\n",
            "Goal count:  0\n",
            "Epsilon:  0.9801986733067553\n",
            "---------------Episode:  11 ---------------\n",
            "End state:  (20, 7)\n",
            "Steps:  324\n",
            "Goal count:  0\n",
            "Epsilon:  0.97824023505121\n",
            "---------------Episode:  12 ---------------\n",
            "End state:  (21, 9)\n",
            "Steps:  211\n",
            "Goal count:  0\n",
            "Epsilon:  0.9762857097579093\n",
            "---------------Episode:  13 ---------------\n",
            "End state:  (14, 18)\n",
            "Steps:  1000\n",
            "Goal count:  0\n",
            "Epsilon:  0.9743350896087494\n",
            "---------------Episode:  14 ---------------\n",
            "End state:  (20, 8)\n",
            "Steps:  567\n",
            "Goal count:  0\n",
            "Epsilon:  0.9723883668012469\n",
            "---------------Episode:  15 ---------------\n",
            "End state:  (24, 8)\n",
            "Steps:  509\n",
            "Goal count:  0\n",
            "Epsilon:  0.9704455335485082\n",
            "---------------Episode:  16 ---------------\n",
            "End state:  (27, 2)\n",
            "Steps:  408\n",
            "Goal count:  0\n",
            "Epsilon:  0.9685065820791976\n",
            "---------------Episode:  17 ---------------\n",
            "End state:  (24, 11)\n",
            "Steps:  865\n",
            "Goal count:  0\n",
            "Epsilon:  0.9665715046375066\n",
            "---------------Episode:  18 ---------------\n",
            "End state:  (20, 8)\n",
            "Steps:  367\n",
            "Goal count:  0\n",
            "Epsilon:  0.9646402934831231\n",
            "---------------Episode:  19 ---------------\n",
            "End state:  (21, 6)\n",
            "Steps:  560\n",
            "Goal count:  0\n",
            "Epsilon:  0.9627129408911995\n",
            "---------------Episode:  20 ---------------\n",
            "End state:  (21, 9)\n",
            "Steps:  416\n",
            "Goal count:  0\n",
            "Epsilon:  0.9607894391523232\n",
            "---------------Episode:  21 ---------------\n",
            "End state:  (20, 8)\n",
            "Steps:  65\n",
            "Goal count:  0\n",
            "Epsilon:  0.9588697805724845\n",
            "---------------Episode:  22 ---------------\n",
            "End state:  (2, 10)\n",
            "Steps:  1000\n",
            "Goal count:  0\n",
            "Epsilon:  0.9569539574730467\n",
            "---------------Episode:  23 ---------------\n",
            "End state:  (27, 20)\n",
            "Steps:  517\n",
            "Goal count:  0\n",
            "Epsilon:  0.9550419621907147\n",
            "---------------Episode:  24 ---------------\n",
            "End state:  (22, 5)\n",
            "Steps:  168\n",
            "Goal count:  0\n",
            "Epsilon:  0.9531337870775047\n",
            "---------------Episode:  25 ---------------\n",
            "End state:  (24, 12)\n",
            "Steps:  121\n",
            "Goal count:  0\n",
            "Epsilon:  0.951229424500714\n",
            "---------------Episode:  26 ---------------\n",
            "End state:  (16, 23)\n",
            "Steps:  257\n",
            "Goal count:  0\n",
            "Epsilon:  0.9493288668428895\n",
            "---------------Episode:  27 ---------------\n",
            "End state:  (20, 8)\n",
            "Steps:  690\n",
            "Goal count:  0\n",
            "Epsilon:  0.9474321065017983\n",
            "---------------Episode:  28 ---------------\n",
            "End state:  (20, 6)\n",
            "Steps:  200\n",
            "Goal count:  0\n",
            "Epsilon:  0.9455391358903963\n",
            "---------------Episode:  29 ---------------\n",
            "End state:  (24, 11)\n",
            "Steps:  295\n",
            "Goal count:  0\n",
            "Epsilon:  0.9436499474367985\n",
            "---------------Episode:  30 ---------------\n",
            "End state:  (20, 6)\n",
            "Steps:  225\n",
            "Goal count:  0\n",
            "Epsilon:  0.9417645335842487\n",
            "---------------Episode:  31 ---------------\n",
            "End state:  (27, 21)\n",
            "Steps:  537\n",
            "Goal count:  0\n",
            "Epsilon:  0.9398828867910889\n",
            "---------------Episode:  32 ---------------\n",
            "End state:  (15, 23)\n",
            "Steps:  164\n",
            "Goal count:  0\n",
            "Epsilon:  0.9380049995307295\n",
            "---------------Episode:  33 ---------------\n",
            "End state:  (27, 3)\n",
            "Steps:  125\n",
            "Goal count:  0\n",
            "Epsilon:  0.9361308642916188\n",
            "---------------Episode:  34 ---------------\n",
            "End state:  (21, 6)\n",
            "Steps:  145\n",
            "Goal count:  0\n",
            "Epsilon:  0.9342604735772135\n",
            "---------------Episode:  35 ---------------\n",
            "End state:  (22, 6)\n",
            "Steps:  177\n",
            "Goal count:  0\n",
            "Epsilon:  0.9323938199059483\n",
            "---------------Episode:  36 ---------------\n",
            "End state:  (20, 7)\n",
            "Steps:  664\n",
            "Goal count:  0\n",
            "Epsilon:  0.9305308958112057\n",
            "---------------Episode:  37 ---------------\n",
            "End state:  (21, 9)\n",
            "Steps:  204\n",
            "Goal count:  0\n",
            "Epsilon:  0.9286716938412872\n",
            "---------------Episode:  38 ---------------\n",
            "End state:  (21, 9)\n",
            "Steps:  355\n",
            "Goal count:  0\n",
            "Epsilon:  0.9268162065593822\n",
            "---------------Episode:  39 ---------------\n",
            "End state:  (20, 6)\n",
            "Steps:  403\n",
            "Goal count:  0\n",
            "Epsilon:  0.9249644265435393\n",
            "---------------Episode:  40 ---------------\n",
            "End state:  (20, 7)\n",
            "Steps:  300\n",
            "Goal count:  0\n",
            "Epsilon:  0.9231163463866358\n",
            "---------------Episode:  41 ---------------\n",
            "End state:  (20, 7)\n",
            "Steps:  100\n",
            "Goal count:  0\n",
            "Epsilon:  0.9212719586963487\n",
            "---------------Episode:  42 ---------------\n",
            "End state:  (21, 9)\n",
            "Steps:  321\n",
            "Goal count:  0\n",
            "Epsilon:  0.9194312560951247\n",
            "---------------Episode:  43 ---------------\n",
            "End state:  (20, 8)\n",
            "Steps:  130\n",
            "Goal count:  0\n",
            "Epsilon:  0.9175942312201509\n",
            "---------------Episode:  44 ---------------\n",
            "End state:  (20, 8)\n",
            "Steps:  214\n",
            "Goal count:  0\n",
            "Epsilon:  0.9157608767233256\n",
            "---------------Episode:  45 ---------------\n",
            "End state:  (20, 7)\n",
            "Steps:  104\n",
            "Goal count:  0\n",
            "Epsilon:  0.9139311852712282\n",
            "---------------Episode:  46 ---------------\n",
            "End state:  (21, 9)\n",
            "Steps:  134\n",
            "Goal count:  0\n",
            "Epsilon:  0.9121051495450904\n",
            "---------------Episode:  47 ---------------\n",
            "End state:  (12, 25)\n",
            "Steps:  99\n",
            "Goal count:  0\n",
            "Epsilon:  0.910282762240767\n",
            "---------------Episode:  48 ---------------\n",
            "End state:  (20, 6)\n",
            "Steps:  166\n",
            "Goal count:  0\n",
            "Epsilon:  0.9084640160687062\n",
            "---------------Episode:  49 ---------------\n",
            "End state:  (26, 12)\n",
            "Steps:  197\n",
            "Goal count:  0\n",
            "Epsilon:  0.9066489037539209\n",
            "---------------Episode:  50 ---------------\n",
            "End state:  (22, 5)\n",
            "Steps:  101\n",
            "Goal count:  0\n",
            "Epsilon:  0.9048374180359595\n",
            "---------------Episode:  51 ---------------\n",
            "End state:  (20, 8)\n",
            "Steps:  228\n",
            "Goal count:  0\n",
            "Epsilon:  0.9030295516688768\n",
            "---------------Episode:  52 ---------------\n",
            "End state:  (20, 8)\n",
            "Steps:  256\n",
            "Goal count:  0\n",
            "Epsilon:  0.9012252974212047\n",
            "---------------Episode:  53 ---------------\n",
            "End state:  (22, 6)\n",
            "Steps:  79\n",
            "Goal count:  0\n",
            "Epsilon:  0.899424648075924\n",
            "---------------Episode:  54 ---------------\n"
          ]
        }
      ],
      "source": [
        "episodes = 1500\n",
        "\n",
        "env = GridEnv(custom_map)\n",
        "\n",
        "agent = Agent(env)\n",
        "\n",
        "policy_net = agent.train_networks(episodes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "id": "5mzMYvOIhSjj"
      },
      "outputs": [],
      "source": [
        "def active_inference(env, open_states, policy_net):\n",
        "    episodes = 1#len(open_states)\n",
        "    score = 0\n",
        "    total_reward = []\n",
        "    average_reward = []\n",
        "    number_of_steps = []\n",
        "    actions = []\n",
        "    not_hit = []\n",
        "    path = []\n",
        "    for k in range(episodes):\n",
        "        num_steps = 0\n",
        "        start_st = (0,0)#open_states[k]\n",
        "        state = env.reset(start_st)\n",
        "        done = False\n",
        "        reward_count = 0\n",
        "        while (not done) and (num_steps < 50):\n",
        "            path.append(state)\n",
        "            #action = np.random.choice([0,1,2,3,4,5,6,7,8], p=Q_pi[i,:, state])\n",
        "            action = sample_action_(state, policy_net)\n",
        "            #print(Q_pi[i,:,state])\n",
        "            actions.append(action.item())\n",
        "            new_state, reward, done, _ = env.step(action.item())\n",
        "            print(\"Reward: \", reward)\n",
        "            reward_count += reward\n",
        "            state = new_state\n",
        "            print(state)\n",
        "            num_steps += 1\n",
        "            if done:\n",
        "                goals = env.return_goal_pos()\n",
        "                if state == goals:\n",
        "                    score += 1\n",
        "                else:\n",
        "                    print(\"FAIL\")\n",
        "                    not_hit.append(start_st)\n",
        "\n",
        "        total_reward.append(reward_count)\n",
        "        average_reward.append(np.mean(total_reward))\n",
        "        number_of_steps.append(num_steps)\n",
        "    \n",
        "    plt.figure(1)\n",
        "    plt.plot(average_reward)\n",
        "    plt.grid()\n",
        "    plt.xlabel(' Open State number')\n",
        "    plt.ylabel('Average reward')\n",
        "\n",
        "    plt.figure(2)\n",
        "    plt.plot(number_of_steps)\n",
        "    plt.grid()\n",
        "    plt.xlabel('State number')\n",
        "    plt.ylabel('Number of steps used to goal')\n",
        "    print(\"Max: \", np.amax(number_of_steps))\n",
        "    print(\"The agent reached the goal {} out of 650 times\".format(score))\n",
        "    print(number_of_steps[0])\n",
        "    print(not_hit)\n",
        "\n",
        "    print(\"Actions: \", actions)\n",
        "    print(\"Path: \", path)\n",
        "\n",
        "def sample_action_(obs, policy_net):\n",
        "    obs = torch.from_numpy(np.array([obs[0], obs[1]]).reshape(1,2)).float()\n",
        "\n",
        "    actions = policy_net(obs)    # Action distribution given the observation\n",
        "    print(\"Actions: \", actions)\n",
        "    return torch.argmax(actions)  # Sample one action from the action distribution\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "OyOfsh5AVrU8",
        "outputId": "3d0236ec-ca5b-4103-8907-470f54cd72a8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Obstacles:  [(9, 29), (10, 28), (10, 29), (11, 26), (11, 27), (11, 28), (11, 29), (12, 25), (12, 26), (12, 27), (12, 28), (12, 29), (13, 25), (13, 26), (13, 27), (13, 28), (13, 29), (14, 24), (14, 25), (14, 26), (14, 27), (14, 28), (14, 29), (15, 23), (15, 24), (15, 25), (15, 26), (15, 27), (15, 28), (15, 29), (16, 23), (16, 24), (16, 25), (16, 26), (16, 27), (16, 28), (16, 29), (17, 23), (17, 24), (17, 25), (17, 26), (17, 27), (17, 28), (17, 29), (18, 24), (18, 25), (18, 26), (18, 27), (18, 28), (18, 29), (19, 24), (19, 25), (19, 26), (19, 27), (19, 28), (19, 29), (20, 6), (20, 7), (20, 8), (20, 25), (20, 26), (20, 27), (20, 28), (20, 29), (21, 6), (21, 7), (21, 8), (21, 9), (21, 26), (21, 27), (21, 28), (21, 29), (22, 5), (22, 6), (22, 7), (22, 8), (22, 27), (22, 28), (22, 29), (23, 5), (23, 6), (23, 7), (23, 8), (23, 27), (23, 28), (23, 29), (24, 5), (24, 6), (24, 7), (24, 8), (24, 11), (24, 12), (24, 26), (24, 27), (24, 28), (24, 29), (25, 4), (25, 5), (25, 6), (25, 7), (25, 11), (25, 12), (25, 25), (25, 26), (25, 27), (25, 28), (25, 29), (26, 4), (26, 5), (26, 6), (26, 7), (26, 8), (26, 9), (26, 10), (26, 11), (26, 12), (26, 13), (26, 25), (26, 26), (26, 27), (26, 28), (26, 29), (27, 2), (27, 3), (27, 4), (27, 5), (27, 6), (27, 7), (27, 8), (27, 9), (27, 10), (27, 11), (27, 12), (27, 13), (27, 17), (27, 18), (27, 19), (27, 20), (27, 21), (27, 25), (27, 26), (27, 27), (27, 28), (27, 29), (28, 2), (28, 3), (28, 4), (28, 5), (28, 6), (28, 7), (28, 8), (28, 9), (28, 10), (28, 11), (28, 12), (28, 13), (28, 16), (28, 17), (28, 18), (28, 19), (28, 20), (28, 21), (28, 22), (28, 23), (28, 24), (28, 25), (28, 26), (28, 27), (28, 28), (28, 29), (29, 1), (29, 2), (29, 3), (29, 4), (29, 5), (29, 6), (29, 7), (29, 8), (29, 9), (29, 10), (29, 11), (29, 12), (29, 16), (29, 17), (29, 18), (29, 19), (29, 20), (29, 21), (29, 22), (29, 23), (29, 24), (29, 25), (29, 26), (29, 27), (29, 28), (29, 29)]\n",
            "Actions:  tensor([[0.0771, 0.0942, 0.0971, 0.1331, 0.1142, 0.1027, 0.1057, 0.1880, 0.0879]],\n",
            "       grad_fn=<ClampBackward1>)\n",
            "Reward:  0.7\n",
            "(1, 1)\n",
            "Actions:  tensor([[0.0792, 0.1018, 0.0977, 0.1632, 0.0646, 0.0704, 0.0756, 0.2101, 0.1374]],\n",
            "       grad_fn=<ClampBackward1>)\n",
            "Reward:  0.7\n",
            "(2, 2)\n",
            "Actions:  tensor([[0.0870, 0.0968, 0.1006, 0.1636, 0.0769, 0.0804, 0.0756, 0.1922, 0.1270]],\n",
            "       grad_fn=<ClampBackward1>)\n",
            "Reward:  0.7\n",
            "(3, 3)\n",
            "Actions:  tensor([[0.0954, 0.0930, 0.1062, 0.1536, 0.0922, 0.0933, 0.0808, 0.1697, 0.1156]],\n",
            "       grad_fn=<ClampBackward1>)\n",
            "Reward:  0.7\n",
            "(4, 4)\n",
            "Actions:  tensor([[0.1013, 0.0854, 0.1131, 0.1445, 0.1081, 0.0993, 0.0825, 0.1581, 0.1079]],\n",
            "       grad_fn=<ClampBackward1>)\n",
            "Reward:  0.7\n",
            "(5, 5)\n",
            "Actions:  tensor([[0.0940, 0.0837, 0.1217, 0.1421, 0.1061, 0.0978, 0.0893, 0.1578, 0.1074]],\n",
            "       grad_fn=<ClampBackward1>)\n",
            "Reward:  0.7\n",
            "(6, 6)\n",
            "Actions:  tensor([[0.0891, 0.0831, 0.1287, 0.1390, 0.1036, 0.0965, 0.0956, 0.1574, 0.1071]],\n",
            "       grad_fn=<ClampBackward1>)\n",
            "Reward:  0.7\n",
            "(7, 7)\n",
            "Actions:  tensor([[0.0908, 0.0854, 0.1299, 0.1338, 0.1013, 0.0973, 0.0985, 0.1546, 0.1083]],\n",
            "       grad_fn=<ClampBackward1>)\n",
            "Reward:  0.7\n",
            "(8, 8)\n",
            "Actions:  tensor([[0.0928, 0.0878, 0.1296, 0.1297, 0.1004, 0.0991, 0.1011, 0.1508, 0.1085]],\n",
            "       grad_fn=<ClampBackward1>)\n",
            "Reward:  0.7\n",
            "(9, 9)\n",
            "Actions:  tensor([[0.0951, 0.0902, 0.1286, 0.1263, 0.1003, 0.1009, 0.1027, 0.1470, 0.1089]],\n",
            "       grad_fn=<ClampBackward1>)\n",
            "Reward:  0.7\n",
            "(10, 10)\n",
            "Actions:  tensor([[0.0972, 0.0927, 0.1276, 0.1228, 0.0999, 0.1029, 0.1041, 0.1435, 0.1093]],\n",
            "       grad_fn=<ClampBackward1>)\n",
            "Reward:  0.7\n",
            "(11, 11)\n",
            "Actions:  tensor([[0.0992, 0.0953, 0.1265, 0.1191, 0.0994, 0.1042, 0.1054, 0.1411, 0.1097]],\n",
            "       grad_fn=<ClampBackward1>)\n",
            "Reward:  0.7\n",
            "(12, 12)\n",
            "Actions:  tensor([[0.1008, 0.0991, 0.1215, 0.1163, 0.0995, 0.1069, 0.1062, 0.1406, 0.1090]],\n",
            "       grad_fn=<ClampBackward1>)\n",
            "Reward:  0.7\n",
            "(13, 13)\n",
            "Actions:  tensor([[0.1024, 0.1010, 0.1175, 0.1157, 0.1015, 0.1102, 0.1048, 0.1382, 0.1087]],\n",
            "       grad_fn=<ClampBackward1>)\n",
            "Reward:  0.7\n",
            "(14, 14)\n",
            "Actions:  tensor([[0.1039, 0.1026, 0.1136, 0.1159, 0.1034, 0.1128, 0.1026, 0.1365, 0.1087]],\n",
            "       grad_fn=<ClampBackward1>)\n",
            "Reward:  0.7\n",
            "(15, 15)\n",
            "Actions:  tensor([[0.1051, 0.1041, 0.1094, 0.1167, 0.1050, 0.1153, 0.0999, 0.1353, 0.1091]],\n",
            "       grad_fn=<ClampBackward1>)\n",
            "Reward:  0.7\n",
            "(16, 16)\n",
            "Actions:  tensor([[0.1062, 0.1062, 0.1059, 0.1173, 0.1073, 0.1174, 0.0983, 0.1320, 0.1095]],\n",
            "       grad_fn=<ClampBackward1>)\n",
            "Reward:  0.7\n",
            "(17, 17)\n",
            "Actions:  tensor([[0.1066, 0.1066, 0.1053, 0.1176, 0.1083, 0.1175, 0.0979, 0.1305, 0.1096]],\n",
            "       grad_fn=<ClampBackward1>)\n",
            "Reward:  0.7\n",
            "(18, 18)\n",
            "Actions:  tensor([[0.1071, 0.1069, 0.1047, 0.1180, 0.1092, 0.1177, 0.0976, 0.1291, 0.1097]],\n",
            "       grad_fn=<ClampBackward1>)\n",
            "Reward:  0.7\n",
            "(19, 19)\n",
            "Actions:  tensor([[0.1075, 0.1072, 0.1042, 0.1183, 0.1102, 0.1178, 0.0972, 0.1277, 0.1098]],\n",
            "       grad_fn=<ClampBackward1>)\n",
            "Reward:  0.7\n",
            "(20, 20)\n",
            "Actions:  tensor([[0.1080, 0.1076, 0.1036, 0.1187, 0.1111, 0.1179, 0.0969, 0.1264, 0.1098]],\n",
            "       grad_fn=<ClampBackward1>)\n",
            "Reward:  0.7\n",
            "(21, 21)\n",
            "Actions:  tensor([[0.1085, 0.1079, 0.1031, 0.1190, 0.1121, 0.1181, 0.0965, 0.1250, 0.1099]],\n",
            "       grad_fn=<ClampBackward1>)\n",
            "Reward:  0.7\n",
            "(22, 22)\n",
            "Actions:  tensor([[0.1089, 0.1082, 0.1025, 0.1194, 0.1131, 0.1182, 0.0961, 0.1236, 0.1099]],\n",
            "       grad_fn=<ClampBackward1>)\n",
            "Reward:  0.7\n",
            "(23, 23)\n",
            "Actions:  tensor([[0.1093, 0.1086, 0.1018, 0.1198, 0.1141, 0.1184, 0.0958, 0.1222, 0.1099]],\n",
            "       grad_fn=<ClampBackward1>)\n",
            "Reward:  0.7\n",
            "(24, 24)\n",
            "Actions:  tensor([[0.1097, 0.1090, 0.1011, 0.1202, 0.1152, 0.1186, 0.0955, 0.1208, 0.1099]],\n",
            "       grad_fn=<ClampBackward1>)\n",
            "Reward:  -2\n",
            "(25, 25)\n",
            "FAIL\n",
            "Max:  25\n",
            "The agent reached the goal 0 out of 650 times\n",
            "25\n",
            "[(0, 0)]\n",
            "Actions:  [7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7]\n",
            "Path:  [(0, 0), (1, 1), (2, 2), (3, 3), (4, 4), (5, 5), (6, 6), (7, 7), (8, 8), (9, 9), (10, 10), (11, 11), (12, 12), (13, 13), (14, 14), (15, 15), (16, 16), (17, 17), (18, 18), (19, 19), (20, 20), (21, 21), (22, 22), (23, 23), (24, 24)]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "obstacles = env.get_obstacles_and_goal(custom_map)\n",
        "open_states = []\n",
        "\n",
        "for i in range(10):\n",
        "    for j in range(10):\n",
        "        if (i,j) not in obstacles:\n",
        "            open_states.append(j + 10*i)\n",
        "\n",
        "active_inference(env, open_states, policy_net)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Msr2fL10k1nD",
        "outputId": "a5138113-eda5-477c-8c1f-d7a6108398d3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0])\n",
            "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0]])\n",
            "tensor([[0],\n",
            "        [0],\n",
            "        [0],\n",
            "        [0],\n",
            "        [0],\n",
            "        [0],\n",
            "        [0],\n",
            "        [0],\n",
            "        [0],\n",
            "        [0],\n",
            "        [0],\n",
            "        [0],\n",
            "        [0],\n",
            "        [0],\n",
            "        [0],\n",
            "        [0],\n",
            "        [0],\n",
            "        [0],\n",
            "        [0],\n",
            "        [0],\n",
            "        [0],\n",
            "        [0],\n",
            "        [0],\n",
            "        [0],\n",
            "        [0]])\n"
          ]
        }
      ],
      "source": [
        "f = torch.tensor([0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0])\n",
        "print(f)\n",
        "\n",
        "\n",
        "f = f.expand(1,25)\n",
        "print(f)\n",
        "\n",
        "f = torch.transpose(f,0,1)\n",
        "print(f)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "l = [0.4339, 0.7716, 0.8642, 0.6308, 1.0100, 0.7703, 0.3682, 0.5911, 0.4441]\n",
        "s = 0\n",
        "for i in range(len(l)):\n",
        "    s += l[i]\n",
        "\n",
        "print(s)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nYt-BRsrVH24",
        "outputId": "4b494bec-bf70-4707-ec36-71b627b78aca"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5.884199999999999\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "t = np.exp([1.15, -2, -0.5, 0.7, 0.25, -0.5])\n",
        "\n",
        "sm = softmax(t)\n",
        "\n",
        "print(sm)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PVtvwE92KWWi",
        "outputId": "d271d3db-3eff-4981-f91d-21d41f08e360"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.5964983  0.02902678 0.04649822 0.18992632 0.09155216 0.04649822]\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Deep active inference.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNZuck3uhvQlCVVrQLCaQgx",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}